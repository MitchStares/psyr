---
title: "Chapter 6. Introductory Statistics"
output:
  html_document:
    toc: true
    toc_float: true
    theme: cosmo
    highlight: textmate
    css: mystyle.css
    number_sections: true
    pandoc_args: [
      "--number-offset", 6
    ]
---

```{r,echo=FALSE}
rm(list=objects()) # start with a clean workspace
source("knitr_tweaks.R")
```

```{css,echo=FALSE}
h1{
  line-height: 100px;
}
h2{
  line-height: 80px;
}
h3{
  line-height: 60px;
}
```

In this chapter I'm going to provide a quick overview of how to perform common data analysis jobs using R. It's essentially a brief coverage of descriptive statistics and a few standard inferential statistics ($t$-tests, etc). I won't explain the logic of *how* the statistical tests work: there are lots of great stats textbooks out there for that. If you're really desperate you can check out the [Learning Statistics with R](http://www.compcogscisydney.com/learning-statistics-with-r.html) book that I wrote for teaching my intro stats class. 

When writing my first ever set of statistics lectures back in 2011, I scraped a set of data from the web that contained some basic information about every Australian Football League (AFL) game played during the years 1987 to 2010. The data is stored in the `afl` data frame and saved in the `afl24.Rdata` file. We'll use that data set throughout this chapter, so here it is:

```{r} 
load("./data/afl24.Rdata") # load data
head(afl)                  # show the first few rows
```

I'm also going to make use of a couple of different packages in this chapter, so I'll load them now:

```{r}
library(psych) # William Revelle's psychometrics package
library(lsr)   # my "learning statistics with R" package (badly in need of some updating!)
library(car)   # nicer Anova function than the base packages
```

## Descriptive statistics

### Using `summary`

Let's suppose all we want to do is get a quick overview of all the variables in the data frame. The `summary` function^[Note: like `print` and `plot`, `summary` is a generic function and does rather different things depending on what kind of input you give it] provides a very handy way of doing that, as you can see here:
```{r}
summary(afl)
```
The output here is giving a sensible summary of each of the variables. To look at any one of these in a little more detail, let's start by considering the "weekday" variable, `afl$weekday`. This is a categorical variable (i.e., a *factor* in R) and so when we summarise that variable on its own, the output is a frequency table:
```{r}
summary( afl$weekday )
```
So there were a total of `r sum(afl$weekday=="Mon")` games played on a Monday over this time period, `r sum(afl$weekday=="Tue")` games played on a Tuesday, and so on. If we feed it a *numeric* variable, like `afl$attendance` we get a set of numbers describing the overall distribution:^[Just FYI: If we summarise a logical variable using a command like `summary(afl$is.final)` we get something very much like a frequency table. For a character variable the `summary` function doesn't work very well, so you'd need to explicitly tell R to treat the variable as a factor using a command like `summary(as.factor(myvariable))`]
```{r}
summary( afl$attendance )
```
So the output here gives you the range: minimum attendance was `r (min(afl$attendance)) %f% "int"` and the maximum was `r (max(afl$attendance)) %f% "int"`. The mean attendance at an AFL game was `r (mean(afl$attendance)) %f% "int"`, whereas the median was a little smaller at `r (median(afl$attendance)) %f% "int"`. Finally, it also gives you the interquartile range, which runs from  `r (quantile(afl$attendance,.25)) %f% "int"` to `r (quantile(afl$attendance,.75)) %f% "int"`. A more detailed view could be generated using the `hist` function to draw a histogram, as discussed in the graphics chapter:
```{r, echo=FALSE}
hist(
  x = afl$attendance,
  col = "lightblue",
  xlab = "Official Attendance",
  ylab = "Frequency",
  main = "",
  axes = FALSE
)
axis(
  side = 1,
  at = seq(0, 120000, 30000),
  labels = seq(0, 120000, 30000) %f% "char"
)
axis(
  side = 2
)
```


### Using `describe`

The `psych` package contains a handy function called `describe` that provides some additional summary statistics that are commonly reported in psychology. However, the measures that it reports are only really sensible for numeric variables (e.g., what exactly would the "mean" of `afl$venue` be?) so the output looks a little weird when the data set contains non-numeric variables. So to produce a tidy output, it's handy to do something like this:

```{r}
numeric.vars <- sapply(afl, is.numeric)  # I'll explain "sapply" later!
describe( afl[,numeric.vars] )           # only "describe" the numeric vars...
```

The package also contains a `describeBy` function that allows you to report the descriptive statistics by group. So, for instance let's suppose I wanted to calculate statistics separately for home and away games (i.e., `afl$is.final == FALSE`) and finals cames (i.e, `afl$is.final == TRUE`) we could use this command:

```{r}
describeBy(
  x =  afl[,numeric.vars], 
  group = afl$is.final 
)
```
This provides handy information: the average attendance for a home and away game was `r mean(afl$attendance[afl$is.final==FALSE]) %f% "int"` whereas for a finals match the average attendance was  `r mean(afl$attendance[afl$is.final==TRUE]) %f% "int"`.


### Mean and median

Introductory stats textbooks usually suggest that we describe the **central tendency** of a variable in terms of its arithmetic *mean* (i.e., the average value), the *median* (i.e., the middle value) or sometimes the *mode* (i.e., the most frequently occurring value). The mean and median are usually applied to numeric variables, whereas the mode is generally more appropriate for categorical variables. I'll talk about the mode later.

To calculate the mean or median of a variable, R supplies the `mean` and `median` functions. It's also worth noting that you can calculate the trimmed mean (e.g., 10\% trimmed mean is the mean value after removing the highest 10\% and lowest values 10\% of the data) by specifying the `trim` argument to the `mean` function. Here's a few examples:

```{r}
mean( afl$home.score ) # mean score by the home team
mean( afl$home.score, trim = 0.1 )  # 10% trimmed mean
mean( afl$home.score, trim = 0.5 )  # 50% trimmed mean...
median( afl$home.score ) # ...  median *is* the 50% trimmed mean
```

### Standard deviation and quantiles

To calculate the **spread** of a variable we often describe the *standard deviation* or report various *quantiles* of a distribution. The `sd` function will compute standard deviations for you...

```{r}
sd( afl$home.score )  # standard deviation
var( afl$home.score ) # variance, just in case you need this!
```

For quantiles, there are a number of specific functions that calculate "special" quantiles (e.g., the interquartile range is `IQR` and the range is `range`), but for the most part I think it makes sense to work with the `quantile` function that can compute any quantiles you like. Here are some examples:

```{r}
quantile(
  x = afl$home.score,
  probs = c(.25, .75) # interquartile range
)
quantile(
    x = afl$home.score,
    probs = c(0, 1)   # the range
)
quantile(
  x = afl$home.score,
  probs = c(.5)       # the median
)
```
Conventiently, the default value of `probs` is to compute all five of those quantiles so the default behaviour is very sensible:

```{r}
quantile( afl$home.score )
```

### Missing data

Something to note is that the `mean` function (and most of the other functions used for descriptive statistics) is a little fragile when given missing data. If you want to *ignore* missing values in a data set, you have to explicitly tell these functions to do so. Most of the time the name of the argument to do this is `na.rm`. For example:

```{r}
age <- c(30, 22, 24, NA)  # eek, one person didn't give their age
mean(age)                 # this doesn't produce an answer!
mean(age, na.rm = TRUE)   # ignore the missing value & average the others
```


### Tabulating data

When dealing with categorical variables, it's convenient to construct a frequency table. The `table` function^[see also the `xtabs` function] will do this nicely for us. For instance:

```{r}
table( afl$home.team )
```

The output of this function, incidentally is an R table object, an we can calculate the sample *mode* by finding the largest value in this table. Just to make this a little prettier for us, let's define a custom function:

```{r}
modal <- function(x) {
  f <- table(x)    # the frequency table
  m <- f == max(f) # which cases are equal to the maximum frequency
  modes <- f[m]    # return all these cases
  return(modes)
}
```

Here's what happens when we apply that to the `afl$home.team` variable:

```{r}
modal( afl$home.team )
```

So Essendon played more home games than anyone else over this time frame. One reason that it can be useful to use a function like the `modal` one we just wrote is that it's robust in the face of ties:

```{r}
eyes <- c("blue","blue","brown", "brown", "green") # this has two modes!
modal(eyes) # this gives a sensible answer
```

### Cross tabulations

Sometimes it is useful to cross tabulate two categorical variables. This is very easy to do with the `table` function, simply by feeding two categorical variables as the input! Let's suppose I wanted to count the number of times each team played a *home game* on each day of the week. Here's how I would do that:

```{r}
table(afl$home.team, afl$weekday)
```

Note that you can add tables to one another. So if I wasn't interested in home games specifically, I could do this:

```{r}
home.day <- table(afl$home.team, afl$weekday)
away.day <- table(afl$away.team, afl$weekday)
print( home.day + away.day ) 
```

Finally, you can construct three way tables or even higher order tables simply by passing more variable to the `table` function, though the output rapidly becomes hard to read! 


### Aggregating by group

A common task in data analysis is to compute (say) the average value of `attendance` by year. I mentioned one way to do this earlier with the `describeBy` function, but another rather more flexible way is to use `aggregate`. Here's how it works:

```{r}
att.by.year <- aggregate(
  formula = attendance ~ year, # we want to look at "attendance" broken down by "year" 
  data = afl,                  # the data are stored in the afl data frame
  FUN = mean                   # the thing we want to compute is the mean
)
att.by.year

```

That does look like quite the upward trend, which becomes more obvious when we plot the data:
```{r}
plot(
  x = att.by.year$year,
  y = att.by.year$attendance,
  xlab = "Year",
  ylab = "Average Attendance",
  type = "h",
  lwd = 5,
  ylim = c(0,40000),
  main = ""
)
```

### Correlations

The `cor` function in R is a flexible tool for calculating correlations between numeric variables. If I want to calculate the correlation between the score of the home team and the score of the away team, for instance, all I do is pass both variables to the `cor` function:
```{r}
cor( afl$home.score, afl$away.score )
```

There's not much of a relationship between them, as this scatterplot illustrates:

```{r, echo=FALSE}
plot(
  x = afl$home.score,
  y = afl$away.score,
  xlab = "Home Team Score",
  ylab = "Away Team Score",
  main = "",
  pch = 19,
  cex = .8
)
```

By default, the `cor` function computes the Pearson correlation between variables (i.e., default `method = "pearson"), but it's straightforward to compute Spearman rank order correlations:

```{r}
cor( 
  x = afl$home.score, 
  y = afl$away.score,
  method = "spearman"
)
```

The `cor` function can compute pairwise correlations between all variables in a data frame, but it only works if all the variables are numeric. Because the `afl` data has many non-numeric variables, this doesn't work:

```{r,error=TRUE}
cor(afl)
```

However, if you recall from earlier we computed a `numeric.vars` vector that indicates which ones are numeric, this version does work:

```{r}
cor(afl[,numeric.vars])
```

As an aside, if you don't like seeing so many digits in the output, you can print out a nicer summary like this:

```{r}
correlations <- cor(afl[,numeric.vars])
print(correlations, digits = 2)
```

### Computing new variables

Finally, one thing to note is that you can make your analysis more flexible by adding new variables to your data frame. For instance, suppose I'm not actually interested in the correlation between home team and away team, but rather want to look at the correlation between the winning score and the losing score (i.e., is there such a thing as a "high scoring game"?). This is where it can be *very* handy to have a good grasp of the data manipulation tools in R. Here's how we might compute the winning and losing scores using the `apply` function:

```{r}
afl$winning.score <- apply( 
  X = afl[, c("home.score","away.score")], # the home and away scores
  MARGIN = 1,                              # "retain" the first dimension
  FUN = max                                #  max along the other dimension
)

afl$losing.score <- apply( 
  X = afl[, c("home.score","away.score")], # the home and away scores
  MARGIN = 1,                              # "retain" the first dimension
  FUN = min                                #  min along the other dimension
)

head(afl)
```

There are a lot of really nice data manipulation functions in R, particularly in the various packages that make up the `tidyverse` (see [here](https://www.tidyverse.org/) for information). In the meantime, let's correlate our new variables:

```{r}
cor(afl$winning.score, afl$losing.score)
```


So there *is* a relationship between the two, but it's only modest. Here's the scatterplot:

```{r, echo=FALSE}
plot(
  x = afl$winning.score,
  y = afl$losing.score,
  xlab = "Winning Team Score",
  ylab = "Losing Team Score",
  main = "",
  pch = 19,
  cex = .8
)
```

## Confidence intervals

As usual there are many ways to compute the confidence interval of the mean in R. One relatively simple one is with the `ciMean` function in the `lsr` package, which (conveniently) can take a data frame as input and computes confidence intervals for all the numeric variables:

```{r}
ciMean(afl)
```

By default it returns a 95\% confidence interval, but you can adjust the `conf` argument if you want something different. For instance, here's an 80\% confidence interval

```{r}
ciMean(
  x = afl, 
  conf = .8
)
```

You can also give it a single variable as input if you like:

```{r}
ciMean( afl$home.score )
```


## Comparing two means

Does the home team tend to outscore the away team? This requires a **paired samples t-test**:

```{r}
pairedSamplesTTest(~ home.score + away.score, afl)
```

Are finals games lower scoring than home and away games? This requires an **independent samples t-test**:

```{r}
afl$total.score <- afl$home.score + afl$away.score
independentSamplesTTest(total.score ~ is.final, afl)
```

## Categorical associations

Are all teams equally likely to play their home games on every weekday? For that we might consider using a **chi-square test of categorical association**, but as you can see from the output below, a little care is needed:

```{r}
associationTest(~ home.team + weekday, afl)
```

The reason for the warning, of course, is that with so few games played on weekdays, many of the expected cell counts are very small, and that violates one of the assumptions of the chi-square test. So let's create a new variable that collapses these:

```{r}
afl$weekday_small <- as.character(afl$weekday)
weekgames <- afl$weekday_small %in% c("Mon","Tue","Wed","Thu","Fri")
afl$weekday_small[weekgames] <- "M-F"
afl$weekday_small <- as.factor(afl$weekday_small)
```

Now we just have three levels of this factor, corresponding to Saturday games, Sunday games, and weekday games. So if we run the test of association with this version of the variable we no longer get the warning message:
```{r}
associationTest(~ home.team + weekday_small, afl)
```

## Comparing several means

Is there such a thing as a "high scoring ground"? Let's take a look at the average number of points per game at each different ground, only considering grounds that had at least 100 games played during the the time period:

```{r}
venue.use <- table(afl$venue)
majors <- venue.use[venue.use >= 100]

# restrict the data to these games
afl.majors <- afl[ afl$venue %in% names(majors), ]
```

Visually it does look like there might something here:

```{r, echo=FALSE}
score.by.ground <- aggregate(total.score ~ venue, afl.majors, mean)
score.by.ground <- sortFrame(score.by.ground, total.score)
score.by.ground
x <- barplot(score.by.ground$total.score)
text(
  x = x-.2,
  y = 5,
  labels = score.by.ground$venue, 
  pos = 4, 
  srt = 90)
```

A first pass analysis for this would be ANOVA. The underlying statistical model in ANOVA and multiple regression is essentially the same, and the work is done by the `lm` function in R. However, it's generally considered sensible to use the `aov` function in the first instance, because that does a few nice things that come in handy with later analyses.

```{r}
mod <- aov(total.score ~ venue, afl.majors)
```

To analyse it as an ANOVA, the `Anova` function in the `car` package is very nice: 
```{r}
Anova(mod)
```

It seems to be a real thing, but we'll come back to that in a moment because we might have some worries about confounding variables. 

### Post hoc tests

I am not a fan of post hoc tests, even with corrections for Type I error inflation. To see why they drive me nuts, let's run the output of the ANOVA through the `posthocPairwiseT` function. By default it uses the Holm correction, but lets just use the simpler and very conservatice Bonferroni correction: 

```{r}
posthocPairwiseT(mod, p.adjust.method = "bonferroni")
```

My main complaint? I have no idea what this means because I didn't really have any idea what I was looking for. I could certainly run through all these automatically-detected "significant" relationships to see what makes any sense, but I honestly don't know what that would buy me. Basically I'm not sure why I'm calculating a $p$-value (a tool designed to *test hypotheses*) in a situation where I really didn't have *any* hypotheses ahead of time. To my mind this use of hypothesis testing has the effect of eroding the boundary between *confirmatory tests* (where the researcher has a theory ahead of time), and *exploratory analyses* (where we're just looking for interesting patterns). I'm a big fan of doing both things as part of science, of course, I just think they need to be kept clearly separate :-)

But that's a topic for another time. 



## Assessing relationships 

One thing that people commented on a lot during this time period was the fact that the games became lower scoring over time. Is that a real effect, or was it just random noise? 

```{r}
mod <- lm(total.score ~ year, afl)
summary(mod)
```


```{r}
yearly.score <- aggregate(
  formula = total.score ~ year, 
  data = afl, 
  FUN = mean
)
plot(
  x = yearly.score$year,
  y = yearly.score$total.score,
  type = "p",
  pch = 19,
  xlab = "Year",
  ylab = "Average Points per Game"
)
abline(coef = mod$coef)
```


That's pretty clearly a real effect, but it does open up a new line of worries about the last analysis...

### Hierarchical regression

Suppose we're a little paranoid. Maybe the effect of `venue` is spurious: some grounds came into use at different years, and we know there's an effect of `year` on the `total.score`. Similarly, folk wisdom has it that finals games are lower scoring, and those games are disproportionately likely to be played at the MCG. Maybe there's an effect of the size of the crowd? Some stadiums are bigger than others? Maybe there's an effect of weekday, and some venues do indeed get used on different days. Maybe it's an effect of the teams playing, since different teams tend to play at different grounds (especially when considering the home team!) To address this let's dump all those variables into a regression model, and then see if adding `venue` leads to an improvement in fit over and above those. In other words, we'll do a **hierarchical regression**. Here it is in R:

```{r}
mod1 <- lm(total.score ~ year + home.team + away.team + is.final + weekday + attendance, afl.majors)
mod2 <- lm(total.score ~ year + home.team + away.team + is.final + weekday + attendance + venue, afl.majors)
anova(mod2, mod1)
```

Overall it does rather look like there are genuine differences between venues. Though of course there could be many other things we didn't control for!

### Testing a correlation

As an aside, it's often noted that a Pearson correlation is essentially equivalent to a linear regression model with a single predictor. So we should be able to replicate the `total.score ~ year` analysis as a correlation. We use the `cor.test` function to run a hypothesis test here:

```{r}
cor.test(
  x = afl$total.score, 
  y = afl$year
)
```

To see that these are giving the same answer, let's take the raw correlation of $r=-.13$, square it, and compare it to the (unadjusted) $R^2$ value of 0.01739 reported above:

```{r}
r <- -0.1318585
print(r^2)
```
Yes, those are the same!


