---
title: "Backpropagation networks"
output:
  html_document:
    includes:
      in_header: header.html    
    toc: true
    toc_float: true
    theme: flatly
    highlight: textmate
    css: mystyle.css
    number_sections: true
    pandoc_args: [
      "--number-offset", 0
    ]
---

```{r,echo=FALSE,message=FALSE,warning=FALSE}
rm(list=objects()) # start with a clean workspace
source("knitr_tweaks.R")
library(tidyverse,quietly = TRUE)
```

Earlier in these notes I used the [Rescorla-Wagner](./programming.html) model of associative learning as an example of how to implement computational models of cognition in R. In this and later sections, I'll expand the dicussion of models to cover a variety of other models in the field. I'll start with the **backpropagation rule** for learning in connectionist networks. 

## Initial implementation 

- The [iris_recode.csv](./data/iris_recode.csv) file contains the classic iris dat slightly reorganised as purely numeric data ([here](./scripts/iris_recode.R)) is the script that generated it. 
- The first version of the modelling code implements a simple two-layer backpropagation network for the iris data: [iris_twolayer.R](./scripts/iris_twolayer.R)
- The second version of the code implements the same model, but expressing the learning rules as matrix operations in order to speed up the calculations: [iris_twolayer2.R](./scripts/iris_twolayer2.R)

At the moment the scripts don't do anything other than learn a classification rule. The goal for the full exercise will be to examine what the model is learning across the series of "epochs", and consider the relationship between this connectionist network and a probabilistic logistic regression model.

Ideally, I'll also include some more complicated examples of backprop networks.


## Resources

- The Rumelhart et al (1986) paper cached for teaching purposes [here](http://compcogscisydney.org/mm/2018/Rumelhart1986.pdf)
- A nice [summary](http://neuralnetworksanddeeplearning.com/chap2.html)

