---
title: "14. Describing data"
output:
  html_document:
    includes:
      in_header: header.html    
    toc: true
    toc_float: true
    theme: flatly
    highlight: textmate
    css: mystyle.css
    number_sections: true
    pandoc_args: [
      "--number-offset", 14
    ]
---

```{r,echo=FALSE,message = FALSE, warning = FALSE}
rm(list=objects()) # start with a clean workspace
source("knitr_tweaks.R")
library(tidyverse,quietly = TRUE)
```

```{css,echo=FALSE}
h1{
  line-height: 100px;
}
h2{
  line-height: 80px;
}
h3{
  line-height: 60px;
}
```

> Out of every hundred people,<br>
> those who always know better:<br>
> fifty-two.<br>
> <br>
> Unsure of every step:<br>
> almost all the rest.<br>
> <br>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Wislawa Szymborska, [A Word on Statistics](https://www.theatlantic.com/past/docs/unbound/poetry/antholog/szymbors/stats.htm) (translated from the Polish by Joanna Trzeciak)


<!--
> Out of every hundred people,
> those who always know better:
> fifty-two.
> <br>
> Unsure of every step:
> almost all the rest.
> <br>
> Ready to help,
> if it doesn't take long:
> forty-nine.
> <br>
> Always good,
> because they cannot be otherwise:
> four -- well, maybe five.
> <br>
> Able to admire without envy:
> eighteen.
> <br>
> Led to error
> by youth (which passes):
> sixty, plus or minus.
> <br>
> Those not to be messed with:
> four-and-forty.
> <br>
> Living in constant fear
> of someone or something:
> seventy-seven.
> <br>
> Capable of happiness:
> twenty-some-odd at most.
> <br>
> Harmless alone,
> turning savage in crowds:
> more than half, for sure.
> <br>
> Cruel
> when forced by circumstances:
> it's better not to know,
> not even approximately.
> <br>
> Wise in hindsight:
> not many more
> than wise in foresight.
> <br>
> Getting nothing out of life except things:
> thirty
> (though I would like to be wrong).
> <br>
> Balled up in pain
> and without a flashlight in the dark:
> eighty-three, sooner or later.
> <br>
> Those who are just:
> quite a few, thirty-five.
> <br>
> But if it takes effort to understand:
> three.
> <br>
> Worthy of empathy:
> ninety-nine.
> <br>
> Mortal:
> one hundred out of one hundred -- ## 
> a figure that has never varied yet.<br>
> <br>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Wislawa Szymborska, [A Word on Statistics](https://www.theatlantic.com/past/docs/unbound/poetry/antholog/szymbors/stats.htm) (translated from the Polish by Joanna Trzeciak)

-->

In this section I'm going to discuss a few methods for computing different kinds of descriptive statistics in R. I won't talk about data visualisation at all until the next section. In the [prelude to data](./prelude-to-data.html) section I referred to the AFL data set that contains some information about every game played in the Australian Rules Football (AFL) league between 1987 and 2010, available in the [afl.csv](./data/afl.csv) file. As usual, I have **tidyverse** loaded, so I'll use `readr::read_csv`^[A quick explanation if you're unfamiliar with the `::` operator. It's a tool you can use to referring to a function inside a package regardless of whether that package is loaded. So `readr::read_csv` means "use the `read_csv` function in the **readr** package". In this instance **readr** is loaded via **tidyverse** so my actual command doesn't use `::`. However, what you see a lot of people doing in textbooks or online is use this notation as a shorthand way to tell people *where* the function is located. The further we go into these notes the more likely I am to start doing that. It's a pretty convenient way of talking about these things!] to import the data:

```{r, message = FALSE, warning = FALSE, fig.width=7}
afl <- read_csv("./data/afl.csv")
afl
```

This data frame has 4296 cases and 12 variables, so I definitely don't want to be summarising anything manually! 

## Simple summaries

When summarising a variable, there are a variety of different measures that one might want to calculate. Means, medians, standard deviations, skewness, etc. There are so many of these functions out there that I'd never be able to capture all of them briefly, even restricting ourselves to numeric variables. Here is a quick overview of the most common statistics:

- `mean()` calculates the arithmetic mean. If you want a trimmed mean it has an optional `trim` argument
- `median()` calculates the median; more generally, the `quantile()` function computes arbitrary quantiles, specified with the `probs` argument
- `min()` and `max()` are the minimum and maximum; `IQR()` returns the interquartile range and `range()` returns the range
- `sd()` calculates the standard deviation (and `var()` computes the variance)
- `psych::skew()` and `psych::kurtosi()` compute skewness and kurtosis respectively

For example, to return the mean and standard deviations for the `home_score` variable I would use these commands:

```{r}
mean(afl$home_score)
sd(afl$home_score)
```

It's worth noting that many of these functions have an `na.rm` argument (or similar) that specifies how R will handle missing data (i.e., `NA` values). Quite often, the default is to set `na.rm = FALSE` meaning that R will not remove missing cases. That can be useful as a way of discovering that your data has missing values: if one of the values is missing then by definition the mean of all your values must also be missing!

```{r}
age <- c(21, 19, 39, NA, 56)
mean(age)
```

In practice though it's often more an annoyance than an aid, so you'll often want to calculate the mean like this:

```{r}
mean(age, na.rm = TRUE)
```


## Correlations

For a pair of numeric variables, we often want to report the strength and direction of correlations. The `cor` function does this, and by default it reports the Pearson correlation:

```{r}
cor(afl$home_score, afl$away_score)
```

As an aside, I don't like writing the name of the data frame twice within the same command. It's a visual distraction and (in my opinion!) it makes the code harder to read. The **magrittr** package solves this for us. In addition to being the origin of the standard R pipe `%>%`, it contains the `%$%` operator that "exposes" the names of the variables inside a data frame to another function. So lets load the package:

```{r, message=FALSE, warning=FALSE}
library(magrittr)
```

Now here's how I would compute the same correlation using `%$%`:

```{r}
afl %$% cor(home_score, away_score)
```

In one sense the difference is purely aesthetic, but aesthetics really do matter when analysing data. In the piped version, I have the name of the data set on the left, and the thing that I'm calculating from it on the right. That feels intuitive and "normal" to me. The original command mixes everything together and I find myself struggling to read it properly. 

Regardless of which version you prefer, you'll be pleased to know that the `cor` function has a `method` argument that lets you specify whether to calculate the `pearson` correlation (the default), the `spearman` rank-sum correlation, or the `kendall` tau measure. For instance:

```{r}
afl %$% cor(home_score, away_score, method = "spearman")
```

A couple of other things to mention about correlations. 

- The `cor.test` function allows you to conduct a hypothesis test for whether a correlation is statistically significant.
- The `cor` function can take a matrix or data frame as input, but it will produce an error if any of the variables are non-numeric. I've used the `lsr::correlate` function to make my life a little easier in this respect, so if I want a correlation matrix that only considers the numeric variables `lsr::correlate(afl)` works
- The `cor` function doesn't handle situations where one or more variables are categorical. The **psych** package has a variety of tools that will solve this for you. It includes functions `tetrachoric`, `polychoric`, `biserial` and `polyserial` to handle those situations. I won't describe those here - I just want to mention them in case you need them later! 

## Frequency tables

For categorical variables, where you usually want a table of frequencies that tallies the number of times each value appears, the `table` function is your friend:^[Note that in this output the weekdays are printed alphabetically rather than in a sensible order. That's because I imported the data straight from a CSV file and I haven't taken the time to convery `weekday` to a factor (see the [data types](./data-types.html) section). You can reorder the levels of a factor however you like. I wrote my own function `lsr::permuteLevels` that does this; there's also the `forcats::fct_relevel` function and the simpler `relevel` function in base R that helps with this. I'll talk more about this later.]

```{r}
afl %$% table(weekday)
```

Not surprisingly, most football games are played on the weekend. The `table` function is quite flexible, and allows you to compute cross-tabulations between two, three, or more variables simply by adding more variable names. As an example, here is the cross tabulation of `year` by `weekday`:

```{r, output.lines=c(1:10)}
afl %$% table(year, weekday)
```

## Summarising by group

Back in the dark pre-**tidyverse** era it was surprisingly difficult to compute descriptive statistics separately for each group. There is a function in base R called `aggregate()` that lets you do this, but it's a little cumbersome. The way I usually do this now is with the **dplyr** functions `group_by()` and `summarise()`. Let's suppose I want to compute the mean attendance at AFL games broken down by who was playing. Conceptually, what I want to do is *"group by"* two variables namely `home_team` and `away_team`, and *"then"* for all unique combinations of those variables *"summarise"* the data by computing the mean `attendance`. That sounds like a job for `%>%`...

```{r}
match_popularity <- afl %>%
  group_by(home_team, away_team) %>%
  summarise(attend = mean(attendance))
```

The call to `group_by()` creates a "grouped data frame" which doesn't look any different to the original *other* than that R now knows what grouping you want! When this grouped data is piped to `summarise()`, R knows that it is supposed to provide results broken down by the relevant grouping variables. So this is what the command above returns:

```{r}
match_popularity
```
  
In the first row, we see that the average attendance at a match between Adelaide and Brisbane over this period was about 39,000 people. 

## Arranging data

The output from our previous command organised the output alphabetically, which is less than ideal. It is usually more helpful to sort the data set in a more meaningful way using `dplyr::arrange`. To  sort the `match_popularity` table in order of increasing attendance, we  do this:
```{r}
match_popularity %>%
  arrange(attend)
```
Clearly, Fitzroy was not a popular team, which would be the reason they were merged with Brisbane in 1996. To sort in descending order we can do this:
```{r}
match_popularity %>%
  arrange(-attend)
```
Sigh. Collingwood. It's always Collingwood. Moving on.

## Filtering cases

How bad was the situation for Fitzroy? Suppose I just want to look at `match_popularity` for Fitzroy home games. The `dplyr::filter` command lets you do this:
```{r}
match_popularity %>%
  filter(home_team == "Fitzroy")
```

The `filter` function can handle more complex logical expressions too. To capture all matches where Fitzroy were playing, regardless of whether they were home or away, a command like this will work: 

```{r, eval = FALSE}
match_popularity %>%
  filter(home_team=="Fitzroy" | away_team =="Fitzroy")
```

However, because this will return a table with 30 rows, R will truncate the output by default. That's mildly annoying in this situation. I'd like to see all the results, preferably sorted in a sensible fashion. To sort the outout we pipe the results to `arrange`, exactly as we did previously, and to ensure that all the rows are printed we can pipe that to an explicit call to `print` that specifies the number of rows to show. Here's the output:

```{r}
match_popularity %>%
  filter(home_team=="Fitzroy" | away_team =="Fitzroy") %>%
  arrange(attend) %>%
  print(n = 30)
```

Poor Fitzroy `r emo::ji("sad")`

## Selecting variables

Many data analysis situations present you with a *lot* of variables to work with, and you might need to `select` a few variables to look at. It's not such a big deal for the AFL data as there are only 12 variables in the raw data, but even there when we print out the tibble it doesn't show us all the cases. Let's suppose we have simple problem where what we want to do is extract the fixture for round 1 of the 1994 season. That is, the only thing we want is the name of the teams playing, the venue and the weekday:

```{r}
afl %>%
  filter(year == 1994 & round == 1) %>%
  select(home_team, away_team, venue, weekday)
```

Well that was easy!


## New variables with mutate

Very often the raw data you are given doesn't contain the actual quantity of iteres, and you might need to compute it from the other variables. The `mutate` function allows you to create new variables within a data frame:

```{r}
afl %>%
  mutate(margin = abs(home_score - away_score)) %>%
  select(home_score, away_score, margin)
```

where the `select` command is just for visual clarity in the output! 
As always, remember that if you want to store the new variable you need to assign the result to a variable. 

<!--
## "Skimming" a data set



```{r}
library(skimr)
skim(afl)
```


-->