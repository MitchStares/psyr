---
title: "12. Prelude to data"
output:
  html_document:
    includes:
      in_header: header.html    
    toc: true
    toc_float: true
    theme: flatly
    highlight: textmate
    css: mystyle.css
    number_sections: true
    pandoc_args: [
      "--number-offset", 12
    ]
---

```{r,echo=FALSE,message = FALSE, warning = FALSE}
rm(list=objects()) # start with a clean workspace
source("knitr_tweaks.R")
library(lsr,quietly = TRUE)
library(tidyverse, quietly = TRUE)
```

When teaching statistics and data analysis, there is a school of thought that argues that the student should be introduced to actual data as quickly as possible. In one sense I've broken that rule rather substantially. The entirety the *core toolkit* section went by without any real data, much less any data analysis. In my defence, data analysis was never the intended goal of that section. My hope with that section was to teach enough programming concepts that you'd be able to implement a psychological model in R, as we did with the [Rescorla-Wagner model](./programming.html) previously. 

In this section I'm going to shift the focus to data analysis. I won't talk much about statistical inference about data (that will come in a later section) though. Instead we'll cover topics like tidying data, summarising data and visualising data. As the focus is now explicitly on data, I'll now work with real data. 

## Attendance at the football

The data I'll look at first is one that I put together as part of the *Learning Statistics with R* book. It consists of information about every game of Australian Rules Football (AFL) played between 1987 and 2010. The data are available in the [afl.csv](./data/afl.csv) file. Let's load the data set and take a look at it:

```{r, message = FALSE, warning = FALSE, fig.width=7}
library(tidyverse)
afl <- read_csv("./data/afl.csv")
afl
```

For the moment let's not worry too much about what this code is doing or what the output means. The gist of the output is pretty clear. We have information about 4296 football vgames. For each game we have recorded 12 variables: the names of the home and away team, the number of points scored by each team, information about when the game was played, the venue where it was played, the official attendance statistics, and whether it was a regular home-and-away game or part of the finals series. 

That's nice, but currently our data takes the form of a table with 4296 rows and 12 columns, and I have no idea what any of it *means*. How do I extract meaningful information out of this data set? Let's say my goal is to investigate how the average attendance at AFL games has changed over the years, and I'd like to look at regular season games separate from finals. To do so, I first summarise the AFL data in terms of the relevant variables: 

```{r, message = FALSE, warning = FALSE, fig.width=7}
attendance <- afl %>% 
  group_by(year, game_type) %>% 
  summarise(attendance = mean(attendance)) 
attendance
```

Again, while you probably won't be completely sure how this code works, the gist of it is pretty clear. I'm taking the raw AFL data, grouping the games by the year and whether they were finals games, and calculting the mean attendance.  This *summary* of the data is a huge improvement over the raw data itself, but it's still just a long list of numbers. Like most people, I don't enjoy looking at numbers - I like pictures. 

As before we can take this `attendance` data and turn it into something nicer. The command to do ths is a little more involved but again the gist of it should be clear:

```{r, message = FALSE, warning = FALSE, fig.width=7}
pic <- attendance %>%
  ggplot(aes(x = year, y = attendance)) +  # year on x-axis, attendance on y-axis
  facet_wrap(~game_type) +                 # separate "facets" for each game_type
  geom_point() +                           # add the data as "points"
  geom_smooth()                            # overlay a smooth regression line
plot(pic) 
```

Yay! We now have something that has *meaning* to a human reader. Attendance at home and away games (right panel) rose smoothly from 1987 to 2010. For finals games (left panel) the attendance is higher throughout but the trend is different. Average attendance declined somewhat until about 2004 and then rose thereafter. Noting that, a data analyst would be prompted to investigate further. 

This brief example highlights several key topics when working with data - we have to import data, summarise data, and visualise it in a fashion that makes intuitive sense to people. My goal in this section is to introduce R tools that we can use to do this. Before diving into  the details, I'll comment on a few things going on in this code.

## The tidyverse

The first line of code in the AFL example is `library(tidyverse)`. Per the description on the [tidyverse](https://www.tidyverse.org/) website,

> The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. 

When I wrote *Learning Statistics with R* to accompany my first statistics class the tidyverse didn't play much of a part, because most of it didn't exist. Over the last several years the tidyverse has had a huge influence on how data analysis in R is typically conducted, so in these notes I'll often use on tools from the tidyverse. From now on, when reading the code presented in these notes you should presume that there's a hidden `library(tidyverse)` command at the start of every page... because there is! `r emo::ji("grinning")`

If you are interested in learning more about the tidyverse and the role it plays in the R data science community, an excellent place to start is with Hadley Wickham and Garrett Grolemund's book [R for Data Science](http://r4ds.had.co.nz/).^[As I type this, I realise that I've unconsciously been influenced by the title. I honestly hadn't intended *R for Psychological Science* to serve as an oblique reference to *R for Data Science*, it just felt like the natural name for the book I wanted to write.] The tidyverse emphasises an approach to data exploration illustrated below:

<img src="./img/prelude_explore.png" width=600px>

(You'll see exactly this image used in a lot of tidyverse related documents)

A typical approach to data analysis involves importing data from an external source, tidying the data and cleaning up any problems with the data set, transforming it in a fashion that extracts the relevant information, visualising the key insights, and so on. Even in the brief analysis I presented above you can see some of this process play out.


## Importing a CSV file

After loading the tidyverse, the next thing I did in my analysis was import the data into R. The raw data itself is stored as a *comma separated value* (CSV) file, a plain text file in which each row represents a single game, with values separated by commas. Here is what the raw data file looks like in a text editor:

<img src="./img/csv_afl.png" width=800px>

To import this data into a nice formay that R can work with, I used the `read_csv` function. The `read_csv` function comes from the **readr** package that is automatically loaded as part of the tidyverse (there is a very similar `read.csv` function that is comes with "base" R, but I've come to prefer the tidyverse version). Here's the line of code again:

```{r, eval=FALSE}
afl <- read_csv("./data/afl.csv")
```

What this does is import the data from the text file shown in the screen shot, assign it to a variable called `afl`. The format of the `afl` variable looks rather like a table. More precisely, it is an example of an R data format called as a **tibble**. Later we'll talk more about tibbles, and the closely related **data frame**. For now, let’s just be happy that we imported the data and that it looks about right.

One thing I will mention is that RStudio provides a nice point-and-click method for importing CSV data. Go to the environment pane, select the "import data set" and then choose "from text (readr)":

<img src="./img/csv_import1.png" width=300px>

After selecting the file you want to import, RStudio will present you with an import data window that looks like this:

<img src="./img/csv_import2.png" width=800px>

This should look fairly familiar to anyone who has imported data from other software such as Excel or SPSS. I won't use that method in these notes because it's easier to type `read_csv` but it's worth noting that the option is there.

## The pipe, `%>%`

There's another thing I want to comment on: some new operators have appeared in this example! Many of the operations are familiar from the last section, such as the equality operator `==` and the assignment operator `<-`. However there are two entirely new operators, `~` and `%>%`, and one familiar operator `+` being used in a novel way: 

- The **tilde** operator `~` is used to define **formulas** in R, which provide a way of describing the relationships between variables. They appear most often in statistical modelling. For instance if I want to build a regression model to describe the relationship between `anxiety` and  demographic variables such as `gender` and `age`, I would express the model conceptually using the formula `anxiety ~ gender + age`. Formulas tend to mean different things in different concepts, but for now it's enough to note that they're an abstract way to describe *relationships*

- The **addition** operator `+` is being used in an unusual way. When I construct a plot by writing `ggplot() + geom_point() + ` etc I'm not literally doing arithmetic, but I'm doing something conceptually similar... adding *layers* to a plot. Under the hood, the **ggplot2** package (part of the tidyverse) has its own special definition of `+` that applies to the plots that it draws. Again, I'll talk more about it later: for now let's just notice that R does allow packages to do clever things like this `r emo::ji("grinning")`

Okay, but what's the story with `%>%`??? 

### The pipe metaphor

The `%>%` operator is called a **pipe**, and is discussed in some detail in the [R for Data Science](http://r4ds.had.co.nz/pipes.html) book. Unlike the operators we've discussed so far (`==`, `&`, etc) the pipe `%>%` isn't part of base R. It was introduced as part of the **magrittr** package, and has become widely adopted.  You'll see it appear in many different contexts. Although **magrittr** isn't loaded as part of the tidyverse, the tidyverse does exported `%>%`, and because you should assume from now on that I have the tidyverse loaded, I'll have access to the pipe for the rest of these notes. 

So what is it, and how does it work?

Let's start by looking at how I used the pipe in the brief analysis above. Here's the command I used to transform the raw `afl` data into the smaller summary data `attendance`:

```{r,eval = FALSE}
attendance <- afl %>% 
  group_by(year, game_type) %>% 
  summarise(attendance = mean(attendance)) 
```

The question is, how does this code work, and what do those `%>%` symbols mean? To see what's happening here, let's strip this code back to its essential features:

```{r,eval = FALSE}
output <- input %>% 
  group_by() %>% 
  summarise() 
```

If we reorganise this so that it all fits in a single line you get a good sense of what's happening here. We take some `input` object that then gets passed to  `group_by()` function, the output of `group_by()` gets passed to `summarise()`, and the final result gets assigned to `output`:

```{r,eval = FALSE}
output <- input %>% group_by() %>% summarise() 
```

For many people -- myself included -- this way of thinking about data analysis is appealing. At a conceptual level, data analysis is often a series of transformations that you apply to a data set, so I often have this intuition that what I'm doing is constructing a conveyor belt... data goes in this side, and meaning comes out on that side! This intuition forms the basis of the"pipe" metaphor, and it becomes obvious if we switch from using the leftward assign `<-` operator to the right assign `->`. We take data as an `input`, pipe it through a series of functions, and out pops something meaningful as our `output`: 


```{r,eval = FALSE}
input %>% group_by() %>% summarise() -> output
```


### An introduction to the pipe


<!--

## Matching cases with `%in%`

Let’s start with a simple example. When my children were little I naturally spent a lot of time watching TV shows like *In the Night Garden*. In the `nightgarden.Rdata` file, I’ve transcribed a short section of the dialogue from the show. The file contains two vectors, `speaker` and `utterance`, and when we take a look at the data,
```{r}
load("./data/nightgarden.Rdata")
print(speaker)
print(utterance)
```


A second useful trick for extracting a subset of a vector is to use the `%in%` operator. It’s actually very similar to the `==` operator, except that you can supply a collection of acceptable values. For instance, suppose I wanted to find those cases when the `utterance` is either `“pip”` or `“oo”`. We can do that like this:

```{r}
utterance %in% c("pip","oo")
```

This in turn allows us to find the `speaker` for all such cases, 

```{r}
speaker[ utterance %in% c("pip","oo") ]
```

-->




<!--

## Generic functions

There’s one other important thing that I omitted when I discussed functions earlier on, and that’s the concept of a **generic function**. The two most notable examples that you’ll see in the next few chapters are `summary` and `plot`, although you’ve already seen an example of one working behind the scenes, and that’s the `print` function. The thing that makes generics different from the other functions is that their behaviour changes, often quite dramatically, depending on the `class` of the input you give it. The easiest way to explain the concept is with an example. With that in mind, lets take a closer look at what the `print` function actually does. I’ll do this by creating a formula, and printing it out in a few different ways. First, let’s stick with what we know:

```{r}
my.formula <- blah ~ blah.blah  # create a variable of class "formula"
print( my.formula )             # print it the normal way
```

So far, there’s nothing very surprising here. But there’s actually a lot going on behind the scenes here. When I type `print(my.formula)`, what actually happens is the `print` function checks the class of the `my.formula` variable. When the function discovers that the variable it’s been given is a formula, it goes looking for a function called `print.formula`, and then delegates the whole business of printing out the variable to the `print.formula` function.^[For readers with a programming background: R has three separate systems for object oriented programming. The earliest system was S3, and it was very informal: generic functions as described here are part of the S3 system. Later on S4 was introduced as a more formal way of doing things. I confess I never learned S4 because it looked tedious. More recently R introduced Reference Classes, which look kind of neat and I should probably learn about them. Discussed [here](http://adv-r.had.co.nz/R5.html) if you're interested.] For what it’s worth, the name for a “dedicated” function like `print.formula` that exists only to be a special case of a generic function like `print` is a **method**, and the name for the process in which the generic function passes off all the hard work onto a method is called **method dispatch**. You won’t need to understand the details at all for this book, but you do need to know the gist of it; if only because a lot of the functions we’ll use are actually generics.

Just to give you a sense of this, let's do something silly and try to bypass the normal workings of the `print` function:

```{r}
print.default( my.formula ) # do something silly by using the wrong method
```

Hm. You can kind of see that it is trying to print out the same formula, but there’s a bunch of ugly low-level details that have also turned up on screen. This is because the `print.default` method doesn’t know anything about formulas, and doesn’t know that it’s supposed to be hiding the obnoxious internal gibberish that R produces sometimes. 

At this stage, this is about as much as we need to know about generic functions and their methods. In fact, you can get through the entire book without learning any more about them than this, so it’s probably a good idea to end this discussion here.

-->

