---
title: "12. Prelude to data"
output:
  html_document:
    includes:
      in_header: header.html    
    toc: true
    toc_float: true
    theme: flatly
    highlight: textmate
    css: mystyle.css
    number_sections: true
    pandoc_args: [
      "--number-offset", 12
    ]
---

```{r,echo=FALSE,message = FALSE, warning = FALSE}
rm(list=objects()) # start with a clean workspace
source("knitr_tweaks.R")
library(lsr,quietly = TRUE)
library(tidyverse,quietly = TRUE)
```

When teaching statistics and data analysis, there is a school of thought that argues that the student should be introduced to actual data as quickly as possible. In one sense I've broken that rule rather substantially. The entirety the *core toolkit* section went by without any real data, much less any data analysis. In my defence, data analysis was never the intended goal of that section. My hope with that section was to teach enough programming concepts that you'd be able to implement a psychological model in R, as we did with the [Rescorla-Wagner model](./programming.html) previously. 

In this section I'm going to shift the focus to data analysis. I won't talk much about statistical inference about data (that will come in a later section) though. Instead we'll cover topics like tidying data, summarising data and visualising data. As the focus is now explicitly on data, I'll now work with real data. 

## Attendance at the football

The data I'll look at first is one that I put together as part of the *Learning Statistics with R* book. It consists of information about every game of Australian Rules Football (AFL) played between 1987 and 2010. The data are available in the [afl.csv](./data/afl.csv) file. Let's load the data set and take a look at it:

```{r, message = FALSE, warning = FALSE, fig.width=7}
library(tidyverse)
afl <- read_csv("./data/afl.csv")
afl
```

For the moment let's not worry too much about what this code is doing or what the output means. The gist of the output is pretty clear. We have information about 4296 football vgames. For each game we have recorded 12 variables: the names of the home and away team, the number of points scored by each team, information about when the game was played, the venue where it was played, the official attendance statistics, and whether it was a regular home-and-away game or part of the finals series. 

That's nice, but currently our data takes the form of a table with 4296 rows and 12 columns, and I have no idea what any of it *means*. How do I extract meaningful information out of this data set? Let's say my goal is to investigate how the average attendance at AFL games has changed over the years, and I'd like to look at regular season games separate from finals. To do so, I first summarise the AFL data in terms of the relevant variables: 

```{r, message = FALSE, warning = FALSE, fig.width=7}
attendance <- afl %>% 
  group_by(year, game_type) %>% 
  summarise(attendance = mean(attendance)) 
attendance
```

Again, while you probably won't be completely sure how this code works, the gist of it is pretty clear. I'm taking the raw AFL data, grouping the games by the year and whether they were finals games, and calculting the mean attendance.  This *summary* of the data is a huge improvement over the raw data itself, but it's still just a long list of numbers. Like most people, I don't enjoy looking at numbers - I like pictures. 

As before we can take this `attendance` data and turn it into something nicer. The command to do ths is a little more involved but again the gist of it should be clear:

```{r, message = FALSE, warning = FALSE, fig.width=7}
pic <- attendance %>%
  ggplot(aes(x = year, y = attendance)) +  # year on x-axis, attendance on y-axis
  facet_wrap(~game_type) +                 # separate "facets" for each game_type
  geom_point() +                           # add the data as "points"
  geom_smooth()                            # overlay a smooth regression line
plot(pic) 
```

Yay! We now have something that has *meaning* to a human reader. Attendance at home and away games (right panel) rose smoothly from 1987 to 2010. For finals games (left panel) the attendance is higher throughout but the trend is different. Average attendance declined somewhat until about 2004 and then rose thereafter. Noting that, a data analyst would be prompted to investigate further. 

This brief example highlights several key topics when working with data - we have to import data, summarise data, and visualise it in a fashion that makes intuitive sense to people. My goal in this section is to introduce R tools that we can use to do this. Before diving into  the details, I'll comment on a few things going on in this code.

## The tidyverse

The first line of code in the AFL example is `library(tidyverse)`. Per the description on the [tidyverse](https://www.tidyverse.org/) website,

> The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. 

When I wrote *Learning Statistics with R* to accompany my first statistics class the tidyverse didn't play much of a part, because most of it didn't exist. Over the last several years the tidyverse has had a huge influence on how data analysis in R is typically conducted, so in these notes I'll very often rely on tools from the tidyverse. From now on, when reading the code presented in these notes you should presume that there's a hidden `library(tidyverse)` command at the start of every page... because there is! `r emo::ji("grinning")`

## Reading a CSV file

In the second line of code I use the `read_csv` function to import the data into R. The command is this one:

```{r, eval=FALSE}
afl <- read_csv("./data/afl.csv")
```

The raw data itself is stored as a *comma separated value* (CSV) file, a plain text file in which each row represents a single game, with values separated by commas. The screenshot below shows the raw data file open in a text editor:

<img src="./img/csv_afl.png" width=800px>

So what this line of code does is import the data from the text file shown in the screen shot, assign it to a variable called `afl`. The format of this AFL variable looks rather like a table, and is an example of an R data formay known as a **tibble**. Later we'll talk more about tibbles, and the closely related **data frame**. For now, let’s just be happy that we imported the data and that it looks about right.

One thing I will mention is that RStudio provides a nice point-and-click method for importing CSV data. Go to the environment pane, select the "import data set" and then choose "from text (readr)", like so^[**readr** is the name of the particular tidyverse package that supplies the `read_csv` function. There is a `read.csv` function that comes with R, but I've come to prefer the tidyverse version.] 


<img src="./img/csv_import1.png" width=300px>

After selecting the file you want to import, RStudio will present you with an import data window that looks like this:

<img src="./img/csv_import2.png" width=800px>

This should look fairly familiar to anyone who has imported data from other software such as Excel or SPSS. I won't use that method in these notes because it's easier to type `read_csv` but it's worth noting that the option is there.


## The pipe, `%>%`

The other thing I want to comment on is that there are some new operators that have appeared in this example. There are a number of operations that will look familiar, such as the equality operator `==` and the assignment operator `<-`. There are two entirely new operators, `~` and `%>%`, and there's one familiar operator `+` that we're using in a novel way. For two of these cases, there's not too much we need to talk about right now...

- The **tilde** operator is used to define **formulas**, which provide a way of describing the relationships between variables. They appear most often in statistical modelling. For instance if I want to build a regression model to describe the relationship between `anxiety` and  demographic variables such as `gender` and `age`, I would express the model conceptually using the formula `anxiety ~ gender + age`. Formulas tend to mean different things in different concepts, but for now it's enough to note that they're an abstract way to describe *relationships*

- The **addition** operator is being used in an unusual way in this code: when I construct a plot by writing `ggplot() + geom_point() + ` etc I'm not literally doing arithmetic, but I'm doing something conceptually similar... adding *layers* to a plot. Under the hood, the **ggplot2** package (part of the tidyverse) has its own special definition of `+` that applies to the plots that it draws. Again, I'll talk more about it later: for now let's just notice that R does allow packages to do clever things like this `r emo::ji("grinning")`

... but what's the story with the `%>%` operator??? 

**WRITE ME**

```{r,eval = FALSE}
afl %>% 
  group_by(year, game_type) %>% 
  summarise(attendance = mean(attendance)) 
```

The pipe is discussed in some detail in Hadley Wickham's [R for Data Science](http://r4ds.had.co.nz/pipes.html) book.


<!--

## Matching cases with `%in%`

Let’s start with a simple example. When my children were little I naturally spent a lot of time watching TV shows like *In the Night Garden*. In the `nightgarden.Rdata` file, I’ve transcribed a short section of the dialogue from the show. The file contains two vectors, `speaker` and `utterance`, and when we take a look at the data,
```{r}
load("./data/nightgarden.Rdata")
print(speaker)
print(utterance)
```


A second useful trick for extracting a subset of a vector is to use the `%in%` operator. It’s actually very similar to the `==` operator, except that you can supply a collection of acceptable values. For instance, suppose I wanted to find those cases when the `utterance` is either `“pip”` or `“oo”`. We can do that like this:

```{r}
utterance %in% c("pip","oo")
```

This in turn allows us to find the `speaker` for all such cases, 

```{r}
speaker[ utterance %in% c("pip","oo") ]
```

-->




<!--

## Generic functions

There’s one other important thing that I omitted when I discussed functions earlier on, and that’s the concept of a **generic function**. The two most notable examples that you’ll see in the next few chapters are `summary` and `plot`, although you’ve already seen an example of one working behind the scenes, and that’s the `print` function. The thing that makes generics different from the other functions is that their behaviour changes, often quite dramatically, depending on the `class` of the input you give it. The easiest way to explain the concept is with an example. With that in mind, lets take a closer look at what the `print` function actually does. I’ll do this by creating a formula, and printing it out in a few different ways. First, let’s stick with what we know:

```{r}
my.formula <- blah ~ blah.blah  # create a variable of class "formula"
print( my.formula )             # print it the normal way
```

So far, there’s nothing very surprising here. But there’s actually a lot going on behind the scenes here. When I type `print(my.formula)`, what actually happens is the `print` function checks the class of the `my.formula` variable. When the function discovers that the variable it’s been given is a formula, it goes looking for a function called `print.formula`, and then delegates the whole business of printing out the variable to the `print.formula` function.^[For readers with a programming background: R has three separate systems for object oriented programming. The earliest system was S3, and it was very informal: generic functions as described here are part of the S3 system. Later on S4 was introduced as a more formal way of doing things. I confess I never learned S4 because it looked tedious. More recently R introduced Reference Classes, which look kind of neat and I should probably learn about them. Discussed [here](http://adv-r.had.co.nz/R5.html) if you're interested.] For what it’s worth, the name for a “dedicated” function like `print.formula` that exists only to be a special case of a generic function like `print` is a **method**, and the name for the process in which the generic function passes off all the hard work onto a method is called **method dispatch**. You won’t need to understand the details at all for this book, but you do need to know the gist of it; if only because a lot of the functions we’ll use are actually generics.

Just to give you a sense of this, let's do something silly and try to bypass the normal workings of the `print` function:

```{r}
print.default( my.formula ) # do something silly by using the wrong method
```

Hm. You can kind of see that it is trying to print out the same formula, but there’s a bunch of ugly low-level details that have also turned up on screen. This is because the `print.default` method doesn’t know anything about formulas, and doesn’t know that it’s supposed to be hiding the obnoxious internal gibberish that R produces sometimes. 

At this stage, this is about as much as we need to know about generic functions and their methods. In fact, you can get through the entire book without learning any more about them than this, so it’s probably a good idea to end this discussion here.

-->

