---
title: "10. Programming"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    highlight: textmate
    css: mystyle.css
    number_sections: true
    pandoc_args: [
      "--number-offset", 10
    ]
---

```{r,echo=FALSE}
rm(list=objects()) # start with a clean workspace
source("knitr_tweaks.R")
```

> *Fry* -- <br>
> Who cares what you're programmed for? <br>
> If someone programmed you to jump off a bridge, would you do it? <br><br>
> *Bender* -- <br>
> I'll have to check my program ... [pause] ... Yep! <br>
> <br>
> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--Futurama, *Space Pilot 3000*, 1999



At this point you have all the tools you need to write a fully functional R program. To illustrate this, lets write a program that implements the [Rescorla-Wagner model](http://www.scholarpedia.org/article/Rescorla-Wagner_model) of associative learning, and apply it to a few simple experimental designs.

## Rescorla-Wagner

The Rescorla-Wagner model provides a learning rule that describes how associative strength changes during Pavlovian conditioning. Suppose we take an initially neutral stimulus (e.g., a tone), and pair it with an outcome that has inherent value to the organism (e.g., food, shock). Over time the organism learns to associate the tone with the shock and will respond to the tone in much the same way that it does to the shock. In this example the shock is referred to as the *unconditioned stimulus* (US) and the tone is the *conditioned stimulus* (CS). 

Suppose we present a compound AB, which consists of two things, a tone (A) and a light (B).

$$
\begin{array}{rcl}
v_A &\leftarrow& v_A + \alpha_A \beta_U (\lambda_U - v_{AB}) \\
v_B &\leftarrow& v_B + \alpha_B \beta_U (\lambda_U - v_{AB}) \\
\end{array}
$$
where
$$
v_{AB} = v_A + v_B
$$

## R implementation

To work out how to write a function that implements the Rescorla-Wagner update rule, the first thing we need to ask ourselves is *what situations do we want to describe?* Do we want to be able to handle stimuli consisting of only a single feature (A), compounds with two features (AB), or compounds that might have any number of features? Do we want it to handle any possible values for the parameters $\alpha$, $\beta$ and $\lambda$ or just some? For the current purposes, I'll try to write something fairly general-purpose!

### The skeleton

To start with, I'll create a skeleton for the function that looks like this:

```{r}
update_RW <- function(value, alpha, beta, lambda) {
}
```

My thinking is that `value` is going to be a vector that specifies the associative strength of association between the US and each element of the CS: that is, it will contain the values $v_A$, $v_B$, etc. Similarly, the `alpha` argument will be a vector that specifies the various salience parameters ($\alpha_A$, $\alpha_B$, etc) associated with the CS. I'm going to assume that there is only ever a single US presented, so we'll assume that the learning rate $\beta$ and the maximum associability $\lambda$ associated with the US are just numbers.

### Make a plan

So now how do I fill out the contents of this function? The first thing I usually do is add some comments to scaffold the rest of my code. Basically I'm making a plan:

```{r}
update_RW <- function(value, alpha, beta, lambda) {
  # compute the value of the compound stimulus
  # compute the prediction error
  # compute the change in strength
  # update the association value
  # return the new value
}
```

### Fill in the pieces

Since the stimulus might be a compound (e.g. AB or ABC), the first thing we need to do is calculate the value ($V_{AB}$) of the compound stimulus. In the Rescorla-Wagner model, the associative strength for the compound is just the sum of the individual strengths, so I can use the `sum` function to add up all the elements of the `value` argument:

```{r}
update_RW <- function(value, alpha, beta, lambda) {
  
  # compute the value of the compound stimulus
  value_compound <- sum(value) 
  
  # compute the prediction error
  # compute the change in strength
  # update the association value
  # return the new value
}
```

The `value_compound` vector plays the same role in my R function that $V_{AB}$ plays in the equations for the Rescorla-Wagner model. However, if we look at the Rescorla-Wagner model, it's clear that the quantity that actually drives learning is the *prediction error*, $\lambda_U - V_{AB}$, namely the difference between the maximum association strength that the US supports and the current association strength for the compound. Well that's easy... it's just subtraction:

```{r}
update_RW <- function(value, alpha, beta, lambda) {
  
  # compute the value of the compound stimulus
  value_compound <- sum(value) 
  
  # compute the prediction error
  prediction_error <- lambda - value_compound
  
  # compute the change in strength
  # update the association value
  # return the new value
}
```

Now we have to multiply everything by $\alpha$ and $\beta$, in order to work out how much learning has occurred. In the Rescorla-Wagner model this is often denoted $\Delta v$. That is:

$$
\begin{array}{rcl}
\Delta v_A &=& \alpha_A \beta_U (\lambda_U - v_{AB}) \\
\Delta v_B &=& \alpha_B \beta_U (\lambda_U - v_{AB}) \\
\end{array}
$$

Within our R function, that's really simple because that's just multiplication:


```{r}
update_RW <- function(value, alpha, beta, lambda) {
  
  # compute the value of the compound stimulus
  value_compound <- sum(value) 
  
  # compute the prediction error
  prediction_error <- lambda - value_compound
  
  # compute the change in strength
  value_change <- alpha * beta * prediction_error 
  
  # update the association value
  # return the new value
}
```

Next, all we need to do is update the `value` and return it!

```{r}
update_RW <- function(value, alpha, beta, lambda) {
  
  # compute the value of the compound stimulus
  value_compound <- sum(value) 
  
  # compute the prediction error
  prediction_error <- lambda - value_compound
  
  # compute the change in strength
  value_change <- alpha * beta * prediction_error 
  
  # update the association value
  value <- value + value_change
  
  # return the new value
  return(value)
}
```

### Tidying

Depending on your personal preferences, you might want to reorganise to make this a little shorter. You could do this by shortening the comments and moving them to the side. You might also want to set some sensible default values, as I have done here:

```{r}
update_RW <- function(value, alpha=.3, beta=.3, lambda=1) {
  value_compound <- sum(value)                    # value of the compound 
  prediction_error <- lambda - value_compound     # prediction error
  value_change <- alpha * beta * prediction_error # change in strength
  value <- value + value_change                   # update value
  return(value)
}
```

## Model predictions

### Conditioning 

```{r}
n_trials <- 20
strength <- rep(0,n_trials)

for(trial in 2:n_trials) {
  strength[trial] <- update_RW( strength[trial-1] )
}
print(strength)
```

You can see in the output that with repeated stimulus presentations, the strength of the association rises quickly. It's a little easier to see what's going on if we draw a picture though. Later on I'll introduce data visualisation with R, but for now here's a simple plotting command that will draw it for us:


```{r}
plot(strength, 
     xlab="Trial Number",
     ylab="Association",
     type="b", pch=19)
```






### Extinction 

```{r}
n_trials <- 50
strength <- rep(0,n_trials)
lambda <- .3

for(trial in 2:n_trials) {
  if(trial > 25) lambda <- 0
  
  strength[trial] <- update_RW(
    value = strength[trial-1],
    lambda = lambda
   )
}

plot(strength, 
     xlab="Trial Number",
     ylab="Association",
     type="b",pch=19)
```


### Blocking


```{r}
n_trials <- 50
strength_A <- rep(0,n_trials)
strength_B <- rep(0,n_trials)
alpha <- c(.3, 0)

for(trial in 2:n_trials) {
  if(trial > 15) alpha <- c(.3, .3)
  
  v_old <- c(strength_A[trial-1], strength_B[trial-1])
  v_new <- update_RW(
    value = v_old,
    alpha = alpha
   )
  
  strength_A[trial] <- v_new[1]
  strength_B[trial] <- v_new[2]
}

plot(strength_A, 
     xlab="Trial Number",
     ylab="Association",
     type="b", pch=19, col="blue")
lines(strength_B, 
     pch=19, col="red",
     type="b")
```



## Done!

If you've been following along yourself you are now officially a computational modeller. Nice work! 

