---
title: "17. Working with text"
output:
  html_document:
    includes:
      in_header: header.html    
    toc: true
    toc_float: true
    theme: flatly
    highlight: textmate
    css: mystyle.css
    number_sections: true
    pandoc_args: [
      "--number-offset", 17
    ]
---

```{r,echo=FALSE,message = FALSE, warning = FALSE}
rm(list=objects()) # start with a clean workspace
source("knitr_tweaks.R")
library(tidyverse,quietly = TRUE)
```

```{css,echo=FALSE}
h1{
  line-height: 100px;
}
h2{
  line-height: 80px;
}
h3{
  line-height: 60px;
}
```


Sometimes your data set is quite text heavy. This can be for a lot of different reasons. Maybe the raw data are actually taken from text sources (e.g., newspaper articles), or maybe your data set contains a lot of free responses to survey questions. Or maybe you just need to reorganise some of the text used to describe nominal scale variables. Regardless of what the reason is, you’ll probably want to know a little bit about how to handle text in R. A few minor examples of this have appeared earlier in these notes (e.g., using `nchar()` to calculate the number of characters in a string). When I wrote *Learning Statistics with R* I included some notes on how to work with text using base R, but overall I've come to the view that the tidyverse approach provided via the **stringr** package is a little more coherent and less problematic for new users, so I'll take that approach here. 

## Preliminaries

- `print` 
- `cat`

## Getting started

The data come from Experiment 1 in a [very cool paper](https://doi.org/10.1073/pnas.0707835105) by Simon Kirby, Hannah Cornish, and Kenny Smith. 

```{r}
chains <- read_csv("./data/kirbyetal2008_exp1_wide.csv")
chain_3 <- chains %>% 
  filter(version == "chain3") %>%
  select(color,shape,motion,starts_with("name")) 
print(chain_3)

input <- chain_3$name_0
output <- chain_3$name_10
```

Excellent! I have a data set `r emo::ji("white_check_mark")` Next, I'll need a string manipulation package. **stringr** is loaded automatically with **tidyverse** so that's another `r emo::ji("white_check_mark")`. Time to get started! The easiest way to approach this is to take a look at the package documentation for **stringr**. As with most tidyverse packages it's clear and thorough, so a large part of what I'm doing here is providing an introduction to their work. To quote straight from the package [vignette](https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html)

>There are four main families of functions in stringr:
>
> - Character manipulation: these functions allow you to manipulate individual characters within the strings in character vectors.
> - Whitespace tools to add, remove, and manipulate whitespace.
> - Locale sensitive operations whose operations will vary from locale to locale.
> - Pattern matching functions. These recognise four engines of pattern description. The most common is regular expressions, but there are three other tools.

In my experience, the first three of these are relatively straightforward and are sufficient to cover a wide range of applied problems. The *pattern maching* functions are a little more challenging, at least when regular expressions are involved, but even there you can achieve a lot with only a modest knowledge (which is lucky for me because regular expressions are painful)


## Flatten strings

Our first job might be just to visually inspect the words that appear in the `output` of the iterated learning chain, and compare them to the words that were present in the `input`. To make things easier on ourselves, we'll use the `str_flatten()` function to *flatten* each of these vectors to a single string. We'll specify `collapse = ", "` so that the output takes the form of a nice comma-separated list of words

```{r}
input %>% str_flatten(collapse = ", ")
output %>% str_flatten(collapse = ", ")
```

## String length

The one of the first things I notice when looking at this output is that they're rather different in length. Even though there are 27 words in each list, it looks like the `output` words are shorter. We can check this by using the `str_length()` function to compute the length of each word. 

```{r}
input %>% str_length() 
output %>% str_length()
```

## Substrings

One thing I might be interested in, either for theoretical reasons (e.g., looking for meaningful prefixes) or practical ones (e.g., convenient plot labels) is extract a smaller subset of the string. A common example is to take the first three characters. We can do this using `str_sub()`

```{r}
output %>% str_sub(start = 1, end = 3)
```

A feature of `str_sub` that sometimes comes in handy is that you can specify negative values here to refer to the distance from the end of the string. If for instance I wanted to extract the last three characters in every word:

```{r}
output %>% str_sub(start = -3, end = -1)
```

## String duplication

I'm not sure why I would want it in this context, but we can duplicate strings using `str_dup()`

```{r}
output %>% str_dup(times = 3)
```

## String truncation and padding

Sometimes it is useful to ensure that every string has the same length. Again, this can be   quite useful when labelling data. For longer strings we might need to truncate it using `str_trunc()` and for shorter strings we might need to pad it with whitespace (or other characters) using `str_pad()`. To give a sense of this:

```{r, results='hold'}
str_pad(string = "Danielle", width = 20)
str_pad(string = "Danielle", width = 20, side = "right")
str_pad(string = "Danielle", width = 20, side = "both")
str_pad(string = "Danielle", width = 20, side = "both", pad = "-")
```

```{r, results='hold'}
str_trunc(string = "Danielle", width = 9)
str_trunc(string = "Danielle", width = 8)
str_trunc(string = "Danielle", width = 7)
str_trunc(string = "Danielle", width = 7, ellipsis = "+++")
```

Truncation and padding work very nicely together:

```{r}
input %>% 
  str_pad(width = 7, side = "right") %>%
  str_trunc(width = 7)
```

## Removing white space

A common problem when dealing with text is white space that appears in the wrong place. For example, suppose someone intended to type the words *tupim*, *tupin* and *poi* as part of the experiment but ended up with additional spaces, leading to this as the raw data:

```{r}
raw <- "  tupim   tupin    poi    "
```

The `str_trim()` function removes all leading and trailing whitespace; `str_squish()` does the same but also compresses any internal whitespace to a single character. Thus:

```{r, results='hold'}
raw
raw %>% str_trim()
raw %>% str_squish()
```

## Wrapping a string

```{r}
input %>% 
  str_flatten(", ") %>% 
  str_wrap(width = 30, indent = 5, exdent = 3) %>% 
  cat()
```



## Locale sensitive functions

There are a collection of functions that manipulate properties of text that vary from one language to the next. So for instance

- `str_to_upper`
- `str_to_lower`
- `str_to_title`
- `str_order`
- `str_sort`

Okay, reading through the vignette further, the `str_to_upper`, `str_to_lower` and `str_to_title` functions do exactly what they say on the tin, but the package documentation is at pains to point out that they will give you different answers depending on where you are, as will sorting functions like `str_order` and `str_sort`. Makes perfect sense: alphabets and the ordering of characters within the alphabets vary from place to place, so it's really nice that these are locale-sensitive. Natural language is messy.








## OLD

- `str_pad`
- `str_trim`
- `str_wrap`
- `str_squish`


```{r}
pp <- janeaustenr::prideprejudice
titletext <- "dfdfgsesdfefasesdfsefsadfsefedå"
```


... and we're back! The `str_pad` function adds whitespace padding to ensure that the string reaches a particular length:

```{r, results='hold'}
str_pad(string = titletext, width = 20)
str_pad(string = titletext, width = 20, side = "right")
str_pad(string = titletext, width = 20, side = "both")
str_pad(string = titletext, width = 20, side = "both", pad = "-")
```

Combining `str_pad` and `str_trunc` means that you can force strings to be exactly a desired length, but padding one and truncating the other. Similarly, you can use `str_trim` to remove padding from one or both sides. For me favourite of these functions, it's a toss up between
(a) `str_squish`, which removes *all* excess white space, whether it be leading, middle or trailing...

```{r}
titletext <- "   Pride      and Prejudice


             by Jane    Austen      "
str_squish(titletext)
```

or (b) `str_wrap`, which tries to modify the white space to ensure that every line is approximately the same width:

```{r}
ppstart <- "Pride and Prejudice by Jane Austen. It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife."
cat(str_wrap(string = ppstart, width = 20))
```

It even lets you specify the `indent` and `exdent` of the output string, so you can have margins!



## Pattern matching tasks

Okay, pattern matching time. My nightmares about `grep` have mostly settled down in my old age, and it's refreshing to read something pertaining to regular expressions without bursting into tears! And I really like the way the pattern matching functions are organised in `stringr`. We have functions that specify *tasks*.

For the sake of helping my own learning, I'll type out what they do:

- `str_detect` finds all strings that match the pattern and returns a logical vector indicating whether they match
- `str_subset` is similar, but it returns the subset of matching strings
- `str_count` returns a numeric vector containing the *number* of times each string matches the pattern
- `str_locate` returns the positions within the string of the first match, and `str_locate_all` returns the positions of all matches string
- `str_extract` returns the matches themselves (first match within a string) and `str_extract_all` returns all of the matches.
- `str_match` and `str_match_all` are similar, but they return the matches split by the groups. (see the vignette for the examples!)
- `str_replace` and `str_replace_all` allow you to manipulate the matches, very much the same way that `sub` and `gsub` do.
- `str_split` breaks a string by a separating character; and there's also the `str_split_fixed` function that breaks it into a fixed number of chunks.

Okay, a lot of that feels very familiar, and it'll probably take a bit of getting used to, but I like the overall structure here. I think after a bit of playing it will be really natural.

## Pattern matching engines

The thing I really like, though, is the fact that the *engine* that defines what counts as a pattern is broken up into convenience functions:

- A `fixed` pattern matches the exact pattern (sequence of bytes)
- A `coll` pattern is a little more flexible, and recognises that the same character (in human reading terms) can be represented in different ways, which is useful if you're using non-English characters
- A `boundary` pattern can match the breaks between words, lines, characters, or sentences. Where by convention `""` is treated as meaning `boundary("character")`
- A `regex` (the default) is a regular expression

That feels much cleaner than what I'm used to.

## Trying out the pattern matching?

Okay so let's see if I understand this. Suppose I want to take the start of *Pride and Prejudice*, as per the `ppstart` string, and split it up by word, character, or sentence:

```{r,results='hold'}
str_split(string = ppstart, pattern = boundary("word"))
str_split(string = ppstart, pattern = boundary("character"))
str_split(string = ppstart, pattern = boundary("sentence"))
```

If I wanted to break it up by vowels, I could define a regular expression to do the job:

```{r}
ppstart %>%
  str_split(pattern = regex("[aeiouAEIOU]")) %>%
  str_wrap(width = 80) %>%
  cat
```

where the calls to `str_wrap` and `cat` are there solely to make the output fit on the blog page, and the call to `regex` is redundant since the `stringr` package assumes by default that the pattern is a regular expression.

If I wanted to disemvowel Jane Austen, it looks like I can use `str_remove` as a handy alias for `str_replace` with `replace = ""`. If I want to get rid of all the vowels rather than just one...

```{r}
ppstart %>%
  str_remove_all(pattern = "[aeiouAEIOU]") %>%
  str_squish %>%
  str_wrap(width = 80) %>%
  cat
```

## Regular expressions


## Playing with Pride and Prejudice

Okay, so now I should be able to do neat things with the whole novel. Let's start by gluing the `pp` vector into one long string, since the line breaks don't seem to appear at any interesting locations

```{r}
library(glue)
pp %<>%
  glue_collapse(sep=" ") %>%
  str_squish
pp %>%
  str_sub(start = 1, end = 500) %>%
  str_wrap(width = 80) %>%
  cat
```


Okay, so now I want to give myself something to *do* with this. I'm tired and sick and I can't think of anything clever, so I'll just try and locate every appearance of the words `"Elizabeth"`, `"Darcy"` and `"Wickham"` in the text:
```{r}
elizabeth <- str_locate_all(pp, fixed("Elizabeth"))[[1]][,1]
darcy <- str_locate_all(pp, fixed("Darcy"))[[1]][,1]
wickham <- str_locate_all(pp, fixed("Wickham"))[[1]][,1]
```
And now I will defy all sense (and offend my own sensibilities) by drawing a graph of this using some extraordinarily unsatisfying code:

```{r}
plot(elizabeth, 1:length(elizabeth), xlab="Position",
  ylab="Number of Appearances", xlim=c(0,str_length(pp)), lwd=2,
  col="red", type="s")
lines(darcy, 1:length(darcy), col="blue", lwd=2, type="s")
lines(wickham, 1:length(wickham), col="green", lwd=2, type="s")
legend(x = "topleft", legend = c("Elizabeth", "Darcy", "Wickham"),
  col = c("red","blue","green"), lwd=2)
```
No surprises there: `Elizabeth` occurs most frequently, and at the most uniform rate. At the start of the book `Darcy` appears just as often, but then there are sections of the book that don't concern him and the function flatlines for a while. `Wickham` is absent more often than not.


## More resources

- The [stringr](https://stringr.tidyverse.org/) package homepage
- [Text Mining with R](https://www.tidytextmining.com/) by Julia Silge and David Robinson

